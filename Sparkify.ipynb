{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Analysis for Sparkify\n",
    "# Content\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "    <ul><li><a href=\"#gather\">Gather data</a></li></ul>\n",
    "    <ul><li><a href=\"#assess_and_clean\">Assess and clean</a></li></ul>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>\n",
    "\n",
    "<a id='intro'></a>\n",
    "# Intro\n",
    "We're analyzing data of a music streaming service named Sparkify. The goal is the prediction of customer turnover (churn). The full dataset is 12 GB of logfile data. Apache Spark as a technology for distributed data processing is used to cope with this amount of data. This workspace uses a 0.1 GB subset of the data.\n",
    "\n",
    "<a id='wrangling'></a>\n",
    "# Data Wrangling\n",
    "<a id='gather'></a>\n",
    "## Gather data\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import count, when, isnan, col, desc, udf, avg, row_number, datediff, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Sparkify') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkify = 'mini_sparkify_event_data.json'\n",
    "# path = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(sparkify)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess_and_clean'></a>\n",
    "## Assess and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 286500 rows and 226 users.\n"
     ]
    }
   ],
   "source": [
    "print('The dataset has {} rows and {} users.'.format(df.count(), df.select('userId').drop_duplicates().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines per user\n",
    "df.groupby('userId').count().orderBy(desc('count')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics: learn about the data and find unexpected entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[:6]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[7:13]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[14:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid/missing data\n",
    "#### userID and sessionID: We find no NaN or Null values  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=58392, auth=0, firstName=8346, gender=8346, itemInSession=0, lastName=8346, length=58392, level=0, location=8346, method=0, page=0, registration=8346, sessionId=0, song=58392, status=0, ts=0, userAgent=8346, userId=0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspired by https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe/44631639#44631639\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... but some empty strings in userID\n",
    "The function 'describe' above shows there is no empty string in `sessionId` but at least one empty string in `userId` (min() is empty string).\n",
    "\n",
    "We notice that for empty `userId`s the auth shows 'Logged Out'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UserId is empty in {} cases.'.format(df.filter(df.userId == '').count()))\n",
    "df.filter(df.userId == '').groupBy('auth').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we find that __the userId is missing _only_ when the user is not logged in__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.userId == '') & (df.auth != 'Logged In')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove lines w/o userId\n",
    "__Define:__<br>\n",
    "filter using `userId != ''`.\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_0 = df.count()\n",
    "df = df.filter(df.userId != '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('auth').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.0%} (count: {}) of the dataset rows are left.'.format(df.count() / lines_0, df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN in other columns\n",
    "NaN check above reveals: song, length, artist are missing the same number of rows & are missing more rows than the userId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} lines are completely filled and there are {} lines where the 'page' is 'NextSong'.\".format(\n",
    "    df.dropna(how='any').count(),\n",
    "    df.filter(~df.song.isNull()).groupBy('page').count().sort(desc('count')).select('count').collect()))\n",
    "\n",
    "print('\\n{:.0%} of the log entries are songs requested.'.format(df.dropna(how='any').count() / df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns song, length, artist are filled when the Sparkify user listens to songs. They are not filled e.g. for log entries \"Add to Playlist\", \"Thumbs Up\", etc.\n",
    "\n",
    "$\\rightarrow$ no problems to tackle with NaN values in other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those with most rows when logged in are candidates for the most heavy users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('userId').count().sort(desc('count')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Churn\n",
    "### Check how to identify churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 52 cancellation events for 225 accounts in the dataset (i.e. 23% churn).\n"
     ]
    }
   ],
   "source": [
    "#df.select('page').dropDuplicates().sort('page').show() # identification step\n",
    "cancel_event_count = df.where(df.page == 'Cancellation Confirmation').count()\n",
    "account_count = df.select('userId').drop_duplicates().count()\n",
    "print('There are {} cancellation events for {} accounts in the dataset (i.e. {:.0%} churn).'.format( \n",
    "    cancel_event_count, account_count, cancel_event_count / account_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ adequate churn share in data to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create churn column\n",
    "__Define:__<br> \n",
    "Identify Churn as `df.page == 'Cancellation Confirmation'`. Create a column `Churn_user` which is 1 for a user with a cancellation confirmation. The cancellation is possible for both paid and free users.\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 users churned\n"
     ]
    }
   ],
   "source": [
    "# create flag churn\n",
    "churn_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "\n",
    "# create churn column: mark event\n",
    "df = df.withColumn('Churn', churn_event(df.page))\n",
    "\n",
    "# create churn column: mark customer (=userId); heavy lifting by Spark, list of userIds easier with Pandas\n",
    "users_churn = df.where(df.Churn == 1).select('userId').drop_duplicates().toPandas().values.tolist()\n",
    "users_churn = list(np.concatenate(users_churn).flat)  \n",
    "df = df.withColumn('Churn_user', when(col('userId').isin(users_churn), 1).otherwise(0))\n",
    "print('{} users churned'.format(df.select('userId').filter(df.userId.isin(users_churn)).dropDuplicates().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:\n",
    "column 'Churn' is 1 only in in cancellation row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['userId', 'firstName', 'ts', 'page', 'level', 'Churn']).where(df.userId == '125').sort('ts').collect() #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd = df.select(['userId', 'Churn_user']).drop_duplicates().groupBy('Churn_user').count().toPandas()\n",
    "\n",
    "plt.figure(figsize = (6, 3))\n",
    "sb.set_style('darkgrid')\n",
    "sb.barplot(y='count', x='Churn_user', data=df_churn_pd)\n",
    "plt.title('User Count by Churn', fontsize=16)\n",
    "plt.xlabel('Churn (1=yes)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### possible extension: consider `Downgrade` events:\n",
    "#= do not consider churn where a user cancels a paid subscription\n",
    "#\n",
    "### create flag downgrade\n",
    "#flag_downgrade_event = udf(lambda x: 1 if x=='Submit Downgrade' else 0, IntegerType())\n",
    "#\n",
    "### create downgraded column\n",
    "#df = df.withColumn('downgraded', flag_downgrade_event('page'))\n",
    "#\n",
    "### create flag free or paid phase\n",
    "#windowval = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "#df = df.withColumn('phase', F.sum('downgraded').over(windowval)) # phase: 1=paid before downgrade, 0=free \n",
    "#\n",
    "##df.select(['userId', 'firstName', 'ts', 'page', 'level', 'phase']).where(df.userId == '30').sort('ts').collect() #9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Songs per User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one approach we divide into users who cancel the usage. This is possible for both paid and free users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Songs per User ... first impression, and then the next step: consider time scaling (e.g. per day) \n",
    "Those who cancel the service did use it considerably on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs by churn:\n",
      " 0    191714\n",
      "1     36394\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Songs per User by Churn:\n",
      " 0      0.689366\n",
      "1    699.884615\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "songs_by_churn = df.filter(df.page == 'NextSong').groupby(df.Churn_user).count().orderBy(df.Churn_user).toPandas()['count']\n",
    "count_by_churn = df.groupBy('Churn').count().orderBy('Churn').toPandas()['count']\n",
    "print('Songs by churn:\\n', songs_by_churn, \n",
    "      '\\n\\nSongs per User by Churn:\\n', songs_by_churn / count_by_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most heavy users of those who cancel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.page == 'NextSong') & (df.Churn_user == 1)).groupby('userId').count().orderBy(desc('count')).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least heavy users of those who cancel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.page == 'NextSong') & (df.Churn_user == 1)).groupby('userId').count().orderBy('count').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of songs per user listened\n",
    "df_pd_0 = df.filter((df.page == 'NextSong') & (df.Churn_user == 0)).groupby('userId').count().toPandas()\n",
    "df_pd_1 = df.filter((df.page == 'NextSong') & (df.Churn_user == 1)).groupby('userId').count().toPandas()\n",
    "\n",
    "sb.distplot(df_pd_0['count'], hist=False, label='no churn')\n",
    "sb.distplot(df_pd_1['count'], hist=False, label='churn')\n",
    "plt.legend()\n",
    "plt.title('Songs per user frequency')\n",
    "plt.xlabel('Number of Songs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ those who cancel typically listen to less songs in their lifetime $\\rightarrow$ check below for the activity per time unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songs Listened per User and Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).day)\n",
    "df = df.withColumn('day', get_day(df.ts))\n",
    "#songs_in_day_overall = df.filter(df.page == 'NextSong').groupby(df.day).count().orderBy(df.day.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-----+\n",
      "|userId|day|count|\n",
      "+------+---+-----+\n",
      "|    10|  3|   63|\n",
      "|    10|  8|   57|\n",
      "+------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_in_day = df.filter(df.page == 'NextSong').groupby([df.userId, df.day]).count().orderBy(df.userId, df.day.cast('float'))\n",
    "songs_in_day.show(2)\n",
    "#df.groupBy('Churn_users').agg(count(col('day')).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days listened & Songs per Day\n",
    "__Define:__\n",
    "aggregate to 1 row per userID to generate usual input for a non time-series model instead of multiple lines per user\n",
    "\n",
    "Days listened = count the days where a user listened to songs\n",
    "\n",
    "\n",
    "average of days where the user listened would be inappropriate (e.g. only listening on 30th of a month would be a higher value than listening every day\n",
    "\n",
    "#### how many songs does the user listen to?\n",
    "The song count per userId might be too skewed for users new to the service vs. not new\n",
    "\n",
    "Therefore: average songs listened to on days where the service is used\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_user = songs_in_day.groupby(['userId']).agg(count(col('day')), \n",
    "                                                  avg(col('count'))).orderBy('userId')\n",
    "songs_user = songs_user.select(col('userId'), \n",
    "                               col('count(day)').alias('days_listened'), \n",
    "                               col('avg(count)').alias('songs_per_day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_user.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-check with separately doing a single aggreagation -> fine\n",
    "songs_user_test = songs_in_day.groupby(['userId']).agg(avg(col('count'))).orderBy('userId')\n",
    "#songs_user = songs_user.select(col('userId'), col('count(day)').alias('days_listened'))\n",
    "songs_user_test.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-check for a heavy user identified above listening to a lot of songs\n",
    "songs_user.where(songs_user.userId == 39).show() # listens on all days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account age\n",
    "How long is the customer using the service already? Idea: Who did not churn for a while is less likely to churn in the next time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registration date is given in all data rows for a customer\n",
    "print('all users have just 1 registration date:', \n",
    "      df.select('userId', 'registration').dropDuplicates().count() == df.select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking one value from 'registration' column to figure out how to cast to date\n",
    "spark.sql(\"select to_date(to_timestamp(1538352117000/1000))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "The account age is the time span the customer is using the service. It's the time between the registration and the dataset date, e.g. the newest date included in the dataset. For customers who churned it ends with the churn event.\n",
    "\n",
    "1. Extract the date (column name: `date`) from the timestamp `df.ts` and the registration date from `df.registration` (column name: `date_reg`). Use the maximum of the timestamp date as `dataset_date`. \n",
    "2. Sort by user and timestamp. Get the last row which contains the churn date in the `date` column. For non-churn users the columns `dataset_date` and `date_reg` are relevant which are the same in all rows for a user.\n",
    "3. Get the date difference in days between the churn date and `date_reg` for churn users, and between `dataset_date` and `date_reg` respectively.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+------+-------------+--------+---------+-----+---------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+-----+----------+---+----------+----------+------------+\n",
      "|        artist|     auth|firstName|gender|itemInSession|lastName|   length|level|       location|method|    page| registration|sessionId|     song|status|           ts|           userAgent|userId|Churn|Churn_user|day|      date|  date_reg|dataset_date|\n",
      "+--------------+---------+---------+------+-------------+--------+---------+-----+---------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+-----+----------+---+----------+----------+------------+\n",
      "|Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|    0|         0|  1|2018-10-01|2018-09-28|  2018-12-03|\n",
      "+--------------+---------+---------+------+-------------+--------+---------+-----+---------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+-----+----------+---+----------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1.\n",
    "# timestamp as date: \n",
    "df = df.withColumn('date', F.to_date(to_timestamp(df.ts/1000)))\n",
    "# registration date:\n",
    "df = df.withColumn('date_reg', F.to_date(F.to_timestamp(df.registration/1000)))\n",
    "# dataset date: use the overall maximum timestamp\n",
    "df = df.withColumn('dataset_date', lit(df.agg(F.max(col('date'))).collect().pop()[0])) # 03.12.2018\n",
    "\n",
    "## 2. get last row:\n",
    "# inspired by https://stackoverflow.com/questions/40889564/pyspark-groupby-and-max-value-selection#40892037\n",
    "w = Window().partitionBy('userId').orderBy(F.desc('ts'))\n",
    "df_age = df.withColumn('rank', F.dense_rank().over(w))\n",
    "df_age = df_age.where(F.col('rank')==1)\n",
    "\n",
    "## 3. date difference\n",
    "df_age = df_age.withColumn('account_age', \n",
    "    when(df_age.userId.isin(users_churn), datediff(df_age.date, df_age.date_reg))\n",
    "    .otherwise(datediff(df_age.dataset_date, df_age.date_reg)))\n",
    "df_age = df_age.select('userId', 'account_age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>date</th>\n",
       "      <th>date_reg</th>\n",
       "      <th>dataset_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>10</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>100</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>100001</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>2018-08-18</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>100002</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>2018-06-25</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>100003</td>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId        date    date_reg dataset_date\n",
       "794       10  2018-11-19  2018-09-28   2018-12-03\n",
       "4008     100  2018-11-30  2018-09-26   2018-12-03\n",
       "4195  100001  2018-10-02  2018-08-18   2018-12-03\n",
       "4413  100002  2018-12-03  2018-06-25   2018-12-03\n",
       "4491  100003  2018-10-11  2018-09-18   2018-12-03"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the last row per user with Pandas \n",
    "df.select('userId', 'date', 'date_reg', 'dataset_date').orderBy('userId').toPandas().groupby('userId').tail(1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|userId|account_age|\n",
      "+------+-----------+\n",
      "|    10|         66|\n",
      "|   100|         68|\n",
      "|100001|         45|\n",
      "+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_age.orderBy('userId').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>account_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userId  account_age\n",
       "0     67          152"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check ok:\n",
    "## non-churn user with 152 days\n",
    "#df_age.where(col('userId')==67).toPandas()\n",
    "## OK checking in raw data: reg_date 2018-07-04 to dataset_date 2018-12-03\n",
    "## 152 days based on https://www.timeanddate.com/date/durationresult.html?d1=4&m1=7&y1=2018&d2=03&m2=12&y2=2018\n",
    "#df.where(col('userId')==67).toPandas().tail(1)\n",
    "\n",
    "# check ok:\n",
    "## churn user with 20 days\n",
    "#df_age.where(col('userId')==51).toPandas()\n",
    "## OK checking in raw data: reg_date 2018-09-27 to cancel_date 2018-10-17\n",
    "#df.where(col('userId')==51).toPandas().tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine data: label churn and static user features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "The logfile stored in `df` has multiple rows per UserId. For any user there are static columns, i.e. which always contain the same information. Examples are `gender` and whether the user churned, i.e. `Churn_user`.\n",
    "\n",
    "Extract static features from `df`. Join it with the metrics calculated above, i.e. `songs_user`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row for each user\n",
    "# inspired by https://sparkbyexamples.com/spark/spark-dataframe-how-to-select-the-first-row-of-each-group/\n",
    "df_by_user = df.withColumn('row', row_number().over(Window.partitionBy('userId').orderBy('userId'))).where(col('row') == 1)\n",
    "\n",
    "# reduce columns\n",
    "df_by_user = df_by_user.select(['userId', 'Churn_user', 'gender'])\n",
    "\n",
    "# join with songs_user\n",
    "df_model = df_by_user.join(songs_user, on=['userId']).join(df_age, on=['userId']).orderBy('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|userId|Churn_user|gender|\n",
      "+------+----------+------+\n",
      "|100010|         0|     F|\n",
      "|200002|         0|     M|\n",
      "|   125|         1|     M|\n",
      "+------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_by_user.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------------+\n",
      "|userId|days_listened|    songs_per_day|\n",
      "+------+-------------+-----------------+\n",
      "|    10|            7|96.14285714285714|\n",
      "|   100|           25|           107.28|\n",
      "|100001|            2|             66.5|\n",
      "+------+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_user.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 row per unique userId: True\n"
     ]
    }
   ],
   "source": [
    "print('1 row per unique userId:', \n",
    "      df_by_user.count() == df.select('userId').drop_duplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-------------+-----------------+-----------+\n",
      "|userId|Churn_user|gender|days_listened|    songs_per_day|account_age|\n",
      "+------+----------+------+-------------+-----------------+-----------+\n",
      "|    10|         0|     M|            7|96.14285714285714|         66|\n",
      "|   100|         0|     M|           25|           107.28|         68|\n",
      "|100001|         1|     F|            2|             66.5|         45|\n",
      "+------+----------+------+-------------+-----------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model_pd = df_model.toPandas()\n",
    "df_model.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 3))\n",
    "sb.countplot( x='Churn_user', hue='gender', data=df_model_pd)\n",
    "plt.title('User Count by Churn and Gender', fontsize=16)\n",
    "plt.xlabel('Churn (1=yes)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model shape:', df_model_pd.shape)\n",
    "df_model_pd.to_csv('sparkify.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Churn_user</th>\n",
       "      <th>gender</th>\n",
       "      <th>days_listened</th>\n",
       "      <th>songs_per_day</th>\n",
       "      <th>account_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>96.142857</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>107.280000</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100001</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  Churn_user gender  days_listened  songs_per_day  account_age\n",
       "0      10           0      M              7      96.142857           66\n",
       "1     100           0      M             25     107.280000           68\n",
       "2  100001           1      F              2      66.500000           45"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd875c4c898>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sb.boxplot(data=df_model_pd, y='account_age', x='Churn_user')\n",
    "plt.title('Account age by churn (boxplot)', fontsize=16)\n",
    "plt.xlabel('Churn (1=yes)', fontsize=12)\n",
    "plt.ylabel('account age (Days)', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#pd.read_csv('sparkify.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### who Churn listens on less days and less songs per day\n",
    "holds on average and looking at the distribution; the distribution for engangement is more clearly unimodal than for songs listened per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.groupby('Churn_user').agg(avg(col('days_listened'))).orderBy('Churn_user').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pd = df_model.groupby('Churn_user').agg(avg(col('days_listened'))).orderBy('Churn_user').toPandas()\n",
    "sb.barplot(x='Churn_user', y='avg(days_listened)', data=df_model_pd)\n",
    "plt.title('Days Listened by Churn')\n",
    "plt.xlabel('Churn (1=yes)')\n",
    "plt.ylabel('Days Listened');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (7, 4))\n",
    "#ax = fig.add_subplot(111)\n",
    "#(df.retweet_count[df.p1_dog == True]).plot(kind = 'hist', bins = 20, alpha = .4, label = 'Dog identified', xlim = (0,15000))\n",
    "#df.retweet_count[df.p1_dog == False].plot(kind = 'hist', alpha = .4, label = 'Dog not identified')\n",
    "\n",
    "#plt.title('Retweets by Algorithm (ML) Identification', fontsize = 20)\n",
    "#ax.set_xlabel('Retweets', fontsize = 16)\n",
    "#ax.set_ylabel('Number of Tweets', fontsize = 16)\n",
    "#ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pd = df_model.groupby('Churn_user').agg(avg(col('songs_per_day'))).orderBy('Churn_user').toPandas()\n",
    "sb.barplot(x='Churn_user', y='avg(songs_per_day)', data=df_model_pd)\n",
    "plt.title('Songs Listened per Day by Churn')\n",
    "plt.xlabel('Churn (1=yes)')\n",
    "plt.ylabel('Songs per Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of songs per user & day listened\n",
    "df_pd_0 = df_model.filter(df_model.Churn_user == 0).select('songs_per_day').toPandas()\n",
    "df_pd_1 = df_model.filter(df_model.Churn_user == 1).select('songs_per_day').toPandas()\n",
    "\n",
    "sb.distplot(df_pd_0.songs_per_day, hist=False, label='no churn')\n",
    "sb.distplot(df_pd_1.songs_per_day, hist=False, label='churn')\n",
    "plt.legend()\n",
    "plt.title('Songs listened per day (frequency)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_0 = df_model.filter(df_model.Churn_user == 0).select('days_listened').toPandas()\n",
    "df_pd_1 = df_model.filter(df_model.Churn_user == 1).select('days_listened').toPandas()\n",
    "\n",
    "sb.distplot(df_pd_0.days_listened, hist=False, label='no churn')\n",
    "sb.distplot(df_pd_1.days_listened, hist=False, label='churn')\n",
    "plt.legend()\n",
    "plt.title('Engagement: number of days listening to songs (frequency)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not looking at higher frequencies than days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete:\n",
    "# by hour:\n",
    "\n",
    "## udf = user-defined function\n",
    "#get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).hour) \n",
    "#\n",
    "## create column\n",
    "#df = df.withColumn('hour', get_hour(user_log.ts)) # add 'hour' to dataframe\n",
    "## df.head() \n",
    "#\n",
    "## count next song page request by hour\n",
    "#songs_in_hour = df.filter(df.page == 'NextSong').groupby(df.hour).count().orderBy(df.hour.cast('float'))\n",
    "#songs_in_hour.show()\n",
    "#\n",
    "## visualize: transform to pandas\n",
    "#songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "#plt.scatter(songs_in_hour_pd['hour'], songs_in_hour_pd['count'])\n",
    "#plt.xlim(-1, 24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create flag downgrade\n",
    "#flag_downgrade_event = udf(lambda x: 1 if x=='Submit Downgrade' else 0, IntegerType())\n",
    "#\n",
    "### create downgraded column\n",
    "#user_log_valid = user_log_valid.withColumn('downgraded', flag_downgrade_event('page'))\n",
    "#user_log_valid.head()\n",
    "#\n",
    "### create flag free or paid phase\n",
    "#windowval = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0) # thinkof partition as groupBy; look at all preceeding rows but no following rows\n",
    "#user_log_valid = user_log_valid.withColumn('phase', F.sum('downgraded').over(windowval))\n",
    "#user_log_valid.select(['userId', 'firstName', 'ts', 'page', 'level', 'phase']).where(user_log.userId == '1138').sort('ts').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize.\n",
    "\n",
    "Deciding on the best approach: choose approach 2\n",
    "1. two-step optimization: a) obtain performance metrics for different estimators, choose the best, b) search the optimal parameter for the model from (a) with a grid or randomized search crossvalidation\n",
    "2. one-step optimization: try different estimators based on parameter optimization for all of the models\n",
    "\n",
    "$\\rightarrow Approach (1) might not identify the best model as a higher performance boost from (1b) might yield the best model is one which was weaker at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename target column\n",
    "# using standards 'label' and 'features' as describedin docs: https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forests\n",
    "df_model = df_model.withColumnRenamed('Churn_user', 'label')\n",
    "\n",
    "# train-test split\n",
    "train, non_train = df_model.randomSplit([.6, .4], seed=42)\n",
    "test, validate = non_train.randomSplit([.5, .5], seed=42)\n",
    "print('Precise split of all rows:', df_model.count() == train.count() + test.count() + validate.count())\n",
    "\n",
    "# number of records\n",
    "print('rows -- total: {}, train: {}, test: {}, and validation: {}'.format(\n",
    "    df_model.count(), train.count(), test.count(), validate.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline approach: multiple estimators, a single parameter choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model.show(1)\n",
    "# train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline: use pipeline to prevent leaking knowledge from validation/test set\n",
    "indexer = StringIndexer(inputCol='gender', outputCol='d_female')\n",
    "assembler = VectorAssembler(inputCols=['days_listened', 'songs_per_day', 'account_age', 'd_female'], outputCol='features_unscaled')\n",
    "scaler = StandardScaler(inputCol='features_unscaled', outputCol='features', withMean=True)\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "rf = RandomForestClassifier(seed=42)\n",
    "estimator_list = [lr, rf]\n",
    "\n",
    "counter = 1\n",
    "for estimator in estimator_list: # loop and how to obtain the algorithm name inspired by https://github.com/stephanieirvine/Udacity-Data-Scientist-Nanodegree/blob/main/Project%204/Sparkify.ipynb\n",
    "    print('{}/{}: {}'.format(counter, len(estimator_list), estimator.__class__.__name__))\n",
    "    counter += 1\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler, estimator])\n",
    "    start = time.time()\n",
    "    model = pipeline.fit(train)\n",
    "    result = model.transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    print('... ran {}min and yields:'.format(round((time.time() - start) / 60, 1)))\n",
    "    print('f1: {}, and accuracy: {}'.format(\n",
    "        round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2),\n",
    "        round(evaluator.evaluate(result, {evaluator.metricName: 'accuracy'}), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enhanced approach: multiple estimators, gridsearch over parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_LogisticRegression = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# parameter options inspired by https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6\n",
    "paramGrid_RandomForestClassifier = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [2,10]) \\\n",
    "    .build()\n",
    "    # rf.criterion,['gini','entropy']\n",
    "        \n",
    "def create_cv(paramGrid, pipeline):\n",
    "    return CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3) # parallelism=2\n",
    "# f1 metric only available using the Spark MulticlassClassificationEvaluator, so with the binary version https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html?highlight=binaryclassificationevaluator#pyspark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "def evaluate_model(evaluator, result):\n",
    "    ''' Evaluates the cross validation results.\n",
    "    \n",
    "    Input:\n",
    "    evaluator -- instantiated evaluator object, e.g. MulticlassClassificationEvaluator()\n",
    "    result -- cross-validation fit-transformed data\n",
    "    \n",
    "    Output:\n",
    "    prints performance metrics f1, accuracy, and precision\n",
    "    '''\n",
    "    \n",
    "    print('f1: {}'.format(round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2)))\n",
    "    # accuracy and precision\n",
    "    correct_prediction_count, rows_all = result.filter(result.label == result.prediction).count(), result.count()\n",
    "    precision_count, rows_precision = result.filter((result.label == 1) & (result.prediction == 1)).count(), result.filter(result.label == 1).count()\n",
    "    print('accuracy: {:.0%} ({}/{}), and precision: {:.0%} ({}/{})'.format(\n",
    "        correct_prediction_count / rows_all, correct_prediction_count, rows_all,\n",
    "        precision_count / rows_precision, precision_count, rows_precision))\n",
    "    print('... ran {} min.'.format(round((time.time() - start) / 60, 1)))\n",
    "\n",
    "# https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6\n",
    "#num_trees =  random.choice(list(range(50,500)))\n",
    "#depth = random.choice(list(range(2,10)))\n",
    "#criterion = random.choice(['gini','entropy'])\n",
    "# relevant read?: https://towardsdatascience.com/hyperparameters-part-ii-random-search-on-spark-77667e68b606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced search...\n",
    "#paramGrid_LogisticRegression = ParamGridBuilder() \\\n",
    "#    .addGrid(lr.regParam, [0.0]) \\\n",
    "#    .build()\n",
    "\n",
    "# parameter options inspired by https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6\n",
    "#paramGrid_RandomForestClassifier = ParamGridBuilder() \\\n",
    "#    .addGrid(rf.numTrees, [2]) \\\n",
    "#    .build()\n",
    "#    # rf.criterion,['gini','entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for estimator in estimator_list:\n",
    "#    print(eval('paramGrid_' + estimator.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with gridsearch\n",
    "counter = 1\n",
    "best_f1 = 0\n",
    "\n",
    "for estimator in estimator_list: # see credentials above\n",
    "    print('{}/{}: {}'.format(counter, len(estimator_list), estimator.__class__.__name__))\n",
    "    counter += 1\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler, estimator])\n",
    "    \n",
    "    # create the CrossValidator object with the specific parameter grid and pipeline: \n",
    "    cv = create_cv(eval('paramGrid_' + estimator.__class__.__name__), pipeline)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = cv.fit(train)\n",
    "    result = model.transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    f1 = round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2)\n",
    "        model_best = model.bestModel\n",
    "    evaluate_model(evaluator, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_f1)\n",
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to files: can use it without running the training job first\n",
    "# https://stackoverflow.com/questions/29255145/what-is-the-right-way-to-save-load-models-in-spark-pyspark?noredirect=1&lq=1\n",
    "model_best.write().overwrite().save('pyspark_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52625639/unable-to-load-logistic-regression-model-in-spark-2-x\n",
    "model_trained = PipelineModel.load('pyspark_trained_model')\n",
    "\n",
    "# inspired by https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines\n",
    "prediction = model_trained.transform(validate.select(validate.drop('label').columns))\n",
    "selected = prediction.select(prediction.drop('features_unscaled', 'features', 'rawPrediction').columns)\n",
    "#selected.collect()\n",
    "#for row in selected.collect():\n",
    "#    userId, gender, days_listened, songs_per_day, d_female, prob, prediction = row  # type: ignore\n",
    "#    print(userId, prob[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline accuracy: always predict the majority class no churn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(prediction.filter(col('label') == 0).count() / prediction.count(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad results with the small dataset do not mean no hope. More data is expected to be more helpful for better results than improvements in terms of parameter choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate results\n",
    "prediction = model_trained.transform(validate)\n",
    "evaluate_model(evaluator, result=prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on new data\n",
    "In a real-world application we should leave out the newest data, train a model and test it with the newest data. This can be extended rolling backwards. In this analysis we include all history. We pick up a signal which already led to churn. However, when churn will happen in reality but is not observed yet that training observation is labeled as 'no churn'. This weakens the model. We accept this flaw as out of scope for this project in terms of effort yet easily handled with the concepts presented.\n",
    "\n",
    "Here we apply the best model found to the full dataset. This allows to provide a churn probability for all customers in the Sparkify Callcenter Dashboard web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_all = model_trained.transform(df_model)\n",
    "prediction_all = prediction_all.select(prediction.drop('features_unscaled', 'features', 'rawPrediction').columns)\n",
    "evaluate_model(evaluator, result=prediction_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkify = prediction_all.toPandas()\n",
    "sparkify['probability'] = sparkify.probability.apply(lambda x: x[1])\n",
    "sparkify['userId'] = sparkify.userId.apply(int)\n",
    "#sparkify.to_csv('sparkify.csv', index=False)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///sparkify.db')\n",
    "sparkify.to_sql('user_table', engine, index=False, if_exists='replace')\n",
    "\n",
    "print('Check database content... table exists and has entries:', \n",
    "      pd.read_sql('SELECT * FROM user_table', \n",
    "                  con=engine).shape[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkify.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given low amount of features: hardly able to predict churn\n",
    "print(np.corrcoef(sparkify.label, sparkify.probability)[0,1])\n",
    "plt.scatter(x=sparkify.label, y=sparkify.probability);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for row in selected.collect():\n",
    "#    userId, gender, days_listened, songs_per_day, d_female, prob, prediction = row  # type: ignore\n",
    "#    print(userId, prob[1], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rf.explainParams())\n",
    "#cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare against:\n",
    "# https://github.com/stephanieirvine/Udacity-Data-Scientist-Nanodegree/blob/main/Project%204/Sparkify.ipynb\n",
    "# blog: https://medium.com/swlh/predicting-churn-with-pyspark-4c8edc8a19e0\n",
    "\n",
    "# https://towardsdatascience.com/predicting-customer-churn-using-pyspark-6a78a78a8412"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
