{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Analysis for Sparkify\n",
    "# Content\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "    <ul><li><a href=\"#gather\">Gather data</a></li></ul>\n",
    "    <ul><li><a href=\"#assess_and_clean\">Assess and clean</a></li></ul>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>\n",
    "\n",
    "<a id='intro'></a>\n",
    "# Intro\n",
    "We're analyzing data of a music streaming service named Sparkify. The goal is the prediction of customer turnover (churn). The full dataset is 12 GB of logfile data. Apache Spark as a technology for distributed data processing is used to cope with this amount of data. This workspace uses a 0.1 GB subset of the data.\n",
    "\n",
    "<a id='wrangling'></a>\n",
    "# Data Wrangling\n",
    "<a id='gather'></a>\n",
    "## Gather data\n",
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import count, when, isnan, col, desc, udf, avg, row_number, datediff, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Sparkify') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkify = 'mini_sparkify_event_data.json'\n",
    "# path = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(sparkify)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess_and_clean'></a>\n",
    "## Assess and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5e550c914ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The dataset has {} rows and {} users.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('The dataset has {} rows and {} users.'.format(df.count(), df.select('userId').drop_duplicates().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit from both techniques: \n",
    "# aggregate with Spark and present the output using the Pandas dataframe formatting\n",
    "pd.DataFrame(df.head(1), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines per user\n",
    "df.groupby('userId').count().orderBy(desc('count')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics: learn about the data and find unexpected entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[:6]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[7:13]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[14:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid/missing data\n",
    "#### userID and sessionID: We find no NaN or Null values  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe/44631639#44631639\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... but some empty strings in userID\n",
    "The function 'describe' above shows there is no empty string in `sessionId` but at least one empty string in `userId` (min() is empty string).\n",
    "\n",
    "We notice that for empty `userId`s the auth shows 'Logged Out'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UserId is empty in {} cases.'.format(df.filter(df.userId == '').count()))\n",
    "df.filter(df.userId == '').groupBy('auth').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we find that __the userId is missing _only_ when the user is not logged in__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.userId == '') & (df.auth != 'Logged In')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove lines w/o userId\n",
    "__Define:__<br>\n",
    "filter using `userId != ''`.\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_0 = df.count()\n",
    "df = df.filter(df.userId != '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('auth').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.0%} (count: {}) of the dataset rows are left.'.format(df.count() / lines_0, df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN in other columns\n",
    "NaN check above reveals: song, length, artist are missing the same number of rows & are missing more rows than the userId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} lines are completely filled and there are {} lines where the 'page' is 'NextSong'.\".format(\n",
    "    df.dropna(how='any').count(),\n",
    "    df.filter(~df.song.isNull()).groupBy('page').count().sort(desc('count')).select('count').collect()))\n",
    "\n",
    "print('\\n{:.0%} of the log entries are songs requested.'.format(df.dropna(how='any').count() / df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns song, length, artist are filled when the Sparkify user listens to songs. They are not filled e.g. for log entries \"Add to Playlist\", \"Thumbs Up\", etc.\n",
    "\n",
    "$\\rightarrow$ no problems to tackle with NaN values in other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# Exploratory Data Analysis and Feature Engineering\n",
    "Creating per user:\n",
    "- the Churn label, _and the features:_\n",
    "- Days with listening activity\n",
    "- Songs per Day\n",
    "- Account age\n",
    "- Gender\n",
    "\n",
    "This is a concise set of possibly relevant features. The activity per day seems to be a reasonable approach. A possible extension is to check the impact of features with lower or higher frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those with most rows when logged in are candidates for the most heavy users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('userId').count().sort(desc('count')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn\n",
    "#### Check how to identify churn\n",
    "We identify churn as cancelling the usage. This is possible for both paid and free users and thus allows to train the model for both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which page values exist? (not printed for brevity)\n",
    "#df.select('page').dropDuplicates().sort('page').show() \n",
    "\n",
    "cancel_event_count = df.where(df.page == 'Cancellation Confirmation').count()\n",
    "account_count = df.select('userId').drop_duplicates().count()\n",
    "print('There are {} cancellation events for {} accounts in the dataset (i.e. {:.0%} churn).'.format( \n",
    "    cancel_event_count, account_count, cancel_event_count / account_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ adequate churn share in data to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create churn column\n",
    "__Define:__<br> \n",
    "Identify Churn as `df.page == 'Cancellation Confirmation'`. Create a column `Churn_user` which is 1 for a user with a cancellation confirmation. The cancellation is possible for both paid and free users.\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create flag churn\n",
    "churn_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "\n",
    "# create churn column: mark event\n",
    "df = df.withColumn('Churn', churn_event(df.page))\n",
    "\n",
    "# create churn column: mark customer (=userId); heavy lifting by Spark, list of userIds easier with Pandas\n",
    "users_churn = df.where(df.Churn == 1).select('userId').drop_duplicates().toPandas().values.tolist()\n",
    "users_churn = list(np.concatenate(users_churn).flat)  \n",
    "df = df.withColumn('Churn_user', when(col('userId').isin(users_churn), 1).otherwise(0))\n",
    "print('{} users churned'.format(df.select('userId').filter(df.userId.isin(users_churn)).dropDuplicates().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:\n",
    "column 'Churn' is 1 only in in cancellation row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['userId', 'firstName', 'ts', 'page', 'level', 'Churn']).where(df.userId == '125').sort('ts').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the churn user share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd = df.select(['userId', 'Churn_user']).drop_duplicates().groupBy('Churn_user').count().toPandas()\n",
    "\n",
    "base_color = sb.color_palette()[0]\n",
    "plt.figure(figsize = (6, 3))\n",
    "sb.set_style('darkgrid')\n",
    "sb.barplot(y='count', x='Churn_user', data=df_churn_pd, color=base_color)\n",
    "plt.title('User Count by Churn', fontsize=16)\n",
    "plt.xlabel('Churn (1=yes)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### possible extension: consider `Downgrade` events:\n",
    "# This approach does not cover users canceling a free subscription. \n",
    "#\n",
    "### create flag downgrade\n",
    "#flag_downgrade_event = udf(lambda x: 1 if x=='Submit Downgrade' else 0, IntegerType())\n",
    "#\n",
    "### create downgraded column\n",
    "#df = df.withColumn('downgraded', flag_downgrade_event('page'))\n",
    "#\n",
    "### create flag free or paid phase\n",
    "#windowval = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "#df = df.withColumn('phase', F.sum('downgraded').over(windowval)) # phase: 1=paid before downgrade, 0=free "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Songs per User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Songs per User as first impression, used as feature only after time scaling (per day) performed below\n",
    "Those who cancel the service did use it considerably on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_by_churn = df.filter(df.page == 'NextSong').groupby(df.Churn_user).count().orderBy(df.Churn_user).toPandas()['count']\n",
    "count_by_churn = df.groupBy('Churn').count().orderBy('Churn').toPandas()['count']\n",
    "print('Songs by churn:\\n', songs_by_churn, \n",
    "      '\\n\\nSongs per User by Churn:\\n', songs_by_churn / count_by_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped for brevity:\n",
    "## Most heavy users of those who cancel: e.g. user 29\n",
    "# df.filter((df.page == 'NextSong') & (df.Churn_user == 1)).groupby('userId').count().orderBy(desc('count')).show(1)\n",
    "## Least heavy users of those who cancel: e.g. user 125\n",
    "# df.filter((df.page == 'NextSong') & (df.Churn_user == 1)).groupby('userId').count().orderBy('count').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of songs per user listened (no time scaling)\n",
    "df_pd_0 = df.filter((df.page == 'NextSong') & (df.Churn_user == 0)).groupby('userId').count().toPandas()\n",
    "df_pd_1 = df.filter((df.page == 'NextSong') & (df.Churn_user == 1)).groupby('userId').count().toPandas()\n",
    "\n",
    "sb.distplot(df_pd_0['count'], hist=False, label='no churn')\n",
    "sb.distplot(df_pd_1['count'], hist=False, label='churn')\n",
    "plt.legend()\n",
    "plt.title('Songs listened in account lifetime (frequency)', fontsize=15)\n",
    "plt.xlabel('Songs', fontsize=12)\n",
    "plt.ylabel('User Frequency', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Interpreting__ the figure:\n",
    "\n",
    "$\\rightarrow$ Those who cancel typically listen to less songs in their lifetime. $\\rightarrow$ check below for the activity per time unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Days listened & Songs per Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define:__\n",
    "aggregate to 1 row per userID to generate usual input for a non time-series model instead of multiple lines per user\n",
    "\n",
    "Days listened = count the days where a user listened to songs\n",
    "\n",
    "Average of days where the user listened would be inappropriate (e.g. only listening on 30th of a month would be a higher value than listening every day\n",
    "\n",
    "#### how many songs does the user listen to?\n",
    "The song count per userId might be too skewed for users new to the service vs. not new\n",
    "\n",
    "Therefore: average songs listened to on days where the service is used\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column day of month (integer)\n",
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).day)\n",
    "df = df.withColumn('day', get_day(df.ts))\n",
    "\n",
    "# songs per user and day of month\n",
    "songs_in_day = df.filter(df.page == 'NextSong').groupby([df.userId, df.day]) \\\n",
    "                .count().orderBy(df.userId, df.day.cast('float'))\n",
    "\n",
    "# aggregate to days where the user listened to songs + renaming column\n",
    "songs_user = songs_in_day.groupby(['userId']).agg(count(col('day')), \n",
    "                                                  avg(col('count'))).orderBy('userId')\n",
    "songs_user = songs_user.select(col('userId'), \n",
    "                               col('count(day)').alias('days_listened'), \n",
    "                               col('avg(count)').alias('songs_per_day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_user.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-check with separately doing a single aggreagation -> fine\n",
    "songs_user_test = songs_in_day.groupby(['userId']).agg(avg(col('count'))).orderBy('userId')\n",
    "#songs_user = songs_user.select(col('userId'), col('count(day)').alias('days_listened'))\n",
    "songs_user_test.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-check for a heavy user identified above listening to a lot of songs\n",
    "songs_user.where(songs_user.userId == 39).show() # listens on all days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Account age\n",
    "How long is the customer using the service already? Idea: Who did not churn for a while is less likely to churn in the next time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registration date is given in all data rows for a customer\n",
    "print('all users have just 1 registration date:', \n",
    "      df.select('userId', 'registration').dropDuplicates().count() == df.select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking one value from 'registration' column to figure out how to cast to date\n",
    "spark.sql(\"select to_date(to_timestamp(1538352117000/1000))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "The account age is the time span the customer is using the service. It's the time between the registration and the dataset date, e.g. the newest date included in the dataset. For customers who churned it ends with the churn event.\n",
    "\n",
    "1. Extract the date (column name: `date`) from the timestamp `df.ts` and the registration date from `df.registration` (column name: `date_reg`). Use the maximum of the timestamp date as `dataset_date`. \n",
    "2. Sort by user and timestamp. Get the last row which contains the churn date in the `date` column. For non-churn users the columns `dataset_date` and `date_reg` are relevant which are the same in all rows for a user.\n",
    "3. Get the date difference in days between the churn date and `date_reg` for churn users, and between `dataset_date` and `date_reg` respectively.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.\n",
    "# timestamp as date: \n",
    "df = df.withColumn('date', F.to_date(F.to_timestamp(df.ts/1000)))\n",
    "# registration date:\n",
    "df = df.withColumn('date_reg', F.to_date(F.to_timestamp(df.registration/1000)))\n",
    "# dataset date: use the overall maximum timestamp\n",
    "df = df.withColumn('dataset_date', lit(df.agg(F.max(col('date'))).collect().pop()[0])) # 03.12.2018\n",
    "\n",
    "## 2. get last row:\n",
    "# inspired by https://stackoverflow.com/questions/40889564/pyspark-groupby-and-max-value-selection#40892037\n",
    "w = Window().partitionBy('userId').orderBy(F.desc('ts'))\n",
    "df_age = df.withColumn('rank', F.dense_rank().over(w))\n",
    "df_age = df_age.where(F.col('rank')==1)\n",
    "\n",
    "## 3. date difference\n",
    "df_age = df_age.withColumn('account_age', \n",
    "    when(df_age.userId.isin(users_churn), datediff(df_age.date, df_age.date_reg))\n",
    "    .otherwise(datediff(df_age.dataset_date, df_age.date_reg)))\n",
    "df_age = df_age.select('userId', 'account_age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the last row per user with Pandas \n",
    "df.select('userId', 'Churn_user', 'date', 'date_reg', 'dataset_date').orderBy('userId').toPandas().groupby('userId').tail(1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age.orderBy('userId').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ok:\n",
    "## non-churn user with 152 days\n",
    "#df_age.where(col('userId')==67).toPandas()\n",
    "## OK checking in raw data: reg_date 2018-07-04 to dataset_date 2018-12-03\n",
    "## 152 days based on https://www.timeanddate.com/date/durationresult.html?d1=4&m1=7&y1=2018&d2=03&m2=12&y2=2018\n",
    "#df.where(col('userId')==67).toPandas().tail(1)\n",
    "\n",
    "# check ok:\n",
    "## churn user with 20 days\n",
    "#df_age.where(col('userId')==51).toPandas()\n",
    "## OK checking in raw data: reg_date 2018-09-27 to cancel_date 2018-10-17\n",
    "#df.where(col('userId')==51).toPandas().tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine data: label churn and static user features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "The logfile stored in `df` has multiple rows per UserId. For any user there are static columns, i.e. which always contain the same information. Examples are `gender` and whether the user churned, i.e. `Churn_user`.\n",
    "\n",
    "Extract static features from `df`. Join it with the metrics calculated above, i.e. `songs_user` and `df_age`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row for each user\n",
    "# inspired by https://sparkbyexamples.com/spark/spark-dataframe-how-to-select-the-first-row-of-each-group/\n",
    "df_by_user = df.withColumn('row', row_number().over(Window.partitionBy('userId').orderBy('userId'))).where(col('row') == 1)\n",
    "\n",
    "# reduce columns\n",
    "df_by_user = df_by_user.select(['userId', 'Churn_user', 'gender'])\n",
    "\n",
    "# join with songs_user\n",
    "df_model = df_by_user.join(songs_user, on=['userId']).join(df_age, on=['userId']).orderBy('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_user.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1 row per unique userId:', \n",
    "      df_by_user.count() == df.select('userId').drop_duplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pd = df_model.toPandas()\n",
    "df_model.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 3))\n",
    "sb.countplot(x='Churn_user', hue='gender', data=df_model_pd)\n",
    "plt.title('User Count by Churn and Gender', fontsize=16)\n",
    "plt.xlabel('Churn (1=yes)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sb.boxplot(data=df_model_pd, y='account_age', x='Churn_user', color=base_color)\n",
    "plt.title('Account age by churn (boxplot)', fontsize=16)\n",
    "plt.xlabel('Churn (1=yes)', fontsize=12)\n",
    "plt.ylabel('account age (Days)', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#pd.read_csv('sparkify.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### who Churn listens on less days and less songs per day\n",
    "holds on average and looking at the distribution; the distribution for engangement is more clearly unimodal than for songs listened per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.groupby('Churn_user').agg(avg(col('days_listened'))).orderBy('Churn_user').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pd1 = df_model.groupby('Churn_user').agg(avg(col('days_listened'))).orderBy('Churn_user').toPandas()\n",
    "df_model_pd2 = df_model.groupby('Churn_user').agg(avg(col('songs_per_day'))).orderBy('Churn_user').toPandas()\n",
    "\n",
    "# subplotting inspired by https://dev.to/thalesbruno/subplotting-with-matplotlib-and-seaborn-5ei8\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "sb.barplot(ax=ax[0], x='Churn_user', y='avg(days_listened)', data=df_model_pd1, color=base_color)\n",
    "ax[0].set_title('Days Listened by Churn', fontsize=16)\n",
    "ax[0].set_xlabel('Churn (1=yes)', fontsize=12)\n",
    "ax[0].set_ylabel('Days Listened', fontsize=12);\n",
    "\n",
    "sb.barplot(ax=ax[1], x='Churn_user', y='avg(songs_per_day)', data=df_model_pd2, color=base_color)\n",
    "ax[1].set_title('Songs Listened per Day by Churn', fontsize=16)\n",
    "ax[1].set_xlabel('Churn (1=yes)', fontsize=12)\n",
    "ax[1].set_ylabel('Songs per Day', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of songs per user & day listened\n",
    "df_pd_0 = df_model.filter(df_model.Churn_user == 0).select('songs_per_day').toPandas()\n",
    "df_pd_1 = df_model.filter(df_model.Churn_user == 1).select('songs_per_day').toPandas()\n",
    "\n",
    "## as histogram:\n",
    "#sb.distplot(df_pd_0.songs_per_day, hist=False, label='no churn')\n",
    "#sb.distplot(df_pd_1.songs_per_day, hist=False, label='churn')\n",
    "#plt.legend()\n",
    "#plt.title('Songs Listened per day (frequency)')\n",
    "#plt.xlabel('Songs')\n",
    "#plt.ylabel('User Frequency');\n",
    "\n",
    "fig = plt.figure(figsize = (7, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "df_pd_0.songs_per_day.plot(kind='hist', bins=20, alpha=.4, label='no churn', xlim=(0,200))\n",
    "df_pd_1.songs_per_day.plot(kind='hist', bins=10, alpha=.4, label='churn')\n",
    "\n",
    "plt.title('Songs listened per day', fontsize=18)\n",
    "ax.set_xlabel('Songs', fontsize=15)\n",
    "ax.set_ylabel('Number of Users', fontsize=15)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_0 = df_model.filter(df_model.Churn_user == 0).select('days_listened').toPandas()\n",
    "df_pd_1 = df_model.filter(df_model.Churn_user == 1).select('days_listened').toPandas()\n",
    "\n",
    "## as histogram:\n",
    "#sb.distplot(df_pd_0.days_listened, hist=False, label='no churn')\n",
    "#sb.distplot(df_pd_1.days_listened, hist=False, label='churn')\n",
    "#plt.legend()\n",
    "#plt.title('Engagement: number of days listening to songs (frequency)')\n",
    "#plt.xlabel('Days Listened')\n",
    "#plt.ylabel('User Frequency');\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (7, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "df_pd_0.days_listened.plot(kind='hist', bins=20, alpha=.4, label='no churn', xlim=(0,35))\n",
    "df_pd_1.days_listened.plot(kind='hist', bins=10, alpha=.4, label='churn')\n",
    "\n",
    "plt.title('Number of days listening to songs', fontsize=18)\n",
    "ax.set_xlabel('Days Listened', fontsize=15)\n",
    "ax.set_ylabel('Number of Users', fontsize=15)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional: output the cleaned and feature-enriched model input to csv \n",
    "#print('model shape:', df_model_pd.shape)\n",
    "#df_model_pd.to_csv('sparkify.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## possible extension:\n",
    "## create flag downgrade\n",
    "#flag_downgrade_event = udf(lambda x: 1 if x=='Submit Downgrade' else 0, IntegerType())\n",
    "#\n",
    "### create downgraded column\n",
    "#user_log_valid = user_log_valid.withColumn('downgraded', flag_downgrade_event('page'))\n",
    "#user_log_valid.head()\n",
    "#\n",
    "### create flag free or paid phase\n",
    "#windowval = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0) # thinkof partition as groupBy; look at all preceeding rows but no following rows\n",
    "#user_log_valid = user_log_valid.withColumn('phase', F.sum('downgraded').over(windowval))\n",
    "#user_log_valid.select(['userId', 'firstName', 'ts', 'page', 'level', 'phase']).where(user_log.userId == '1138').sort('ts').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "We have a rather balanced dataset with an imbalance ratio of 4 (225 users in the small dataset / 52 users with a cancellation). This is due to aggregation to the user level before model training. Anyhow, an approach to control recall and precision has no relevant downsides:\n",
    "\n",
    "We could favor recall or precision using an F-beta scorer with $\\beta \\ne 1$. We opt here to use the same weight optimizing the __F1-score__.\n",
    "\n",
    "A baseline estimation predicting the majority class _no churn_ yields an accuracy of 74% using the small 225 customer dataset. Both Logistic Regression and RandomForest yield an improved accuracy of 87%, which is promising.\n",
    "\n",
    "We split in train, test, and validation. \n",
    "\n",
    "We simultneously optimize the algorithm and its parameters. We obtain an F1-score of 87% for the RandomForestClassifier for the test set and ... xxx ... for the validation, respectively.\n",
    "\n",
    "Deciding on the best approach: choose approach 2\n",
    "1. two-step optimization: a) obtain performance metrics for different estimators, choose the best, b) search the optimal parameter for the model from (a) with a grid or randomized search crossvalidation\n",
    "2. one-step optimization: try different estimators based on parameter optimization for all of the models\n",
    "\n",
    "$\\rightarrow Approach (1) __might not identify the best model__ as a higher performance boost from (1b) might yield that the best model is one which was weaker at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename target column\n",
    "# using standards 'label' and 'features' as describedin docs: https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forests\n",
    "df_model = df_model.withColumnRenamed('Churn_user', 'label')\n",
    "\n",
    "# train-test split\n",
    "train, non_train = df_model.randomSplit([.6, .4], seed=42)\n",
    "test, validate = non_train.randomSplit([.5, .5], seed=42)\n",
    "print('Precise split of all rows:', df_model.count() == train.count() + test.count() + validate.count())\n",
    "\n",
    "# number of records\n",
    "print('rows -- total: {}, train: {}, test: {}, and validation: {}'.format(\n",
    "    df_model.count(), train.count(), test.count(), validate.count()))\n",
    "\n",
    "# build pipeline: use pipeline to prevent leaking knowledge from validation/test set\n",
    "indexer = StringIndexer(inputCol='gender', outputCol='d_female')\n",
    "assembler = VectorAssembler(inputCols=['days_listened', 'songs_per_day', 'account_age', 'd_female'], outputCol='features_unscaled')\n",
    "scaler = StandardScaler(inputCol='features_unscaled', outputCol='features', withMean=True)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "rf = RandomForestClassifier(seed=42)\n",
    "estimator_list = [lr, rf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline accuracy: always predict the majority class no churn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(prediction.filter(col('label') == 0).count() / prediction.count(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline approach: multiple estimators, just one parameter choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for estimator in estimator_list:\n",
    "    print('{}/{}: {}'.format(counter, len(estimator_list), estimator.__class__.__name__))\n",
    "    counter += 1\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler, estimator])\n",
    "    start = time.time()\n",
    "    model = pipeline.fit(train)\n",
    "    result = model.transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    print('... ran {}min and yields:'.format(round((time.time() - start) / 60, 1)))\n",
    "    print('f1: {}, and accuracy: {}'.format(\n",
    "        round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2),\n",
    "        round(evaluator.evaluate(result, {evaluator.metricName: 'accuracy'}), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enhanced approach: multiple estimators, gridsearch over parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_LogisticRegression = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# parameter options inspired by https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6\n",
    "paramGrid_RandomForestClassifier = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [2, 10]) \\\n",
    "    .build()\n",
    "    # rf.criterion,['gini','entropy']\n",
    "        \n",
    "def create_cv(paramGrid, pipeline):\n",
    "    return CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3) # parallelism=2\n",
    "# f1 metric only available using the Spark MulticlassClassificationEvaluator, not so with the binary version https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html?highlight=binaryclassificationevaluator#pyspark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "def evaluate_model(evaluator, result):\n",
    "    ''' Evaluates the cross validation results.\n",
    "    \n",
    "    Input:\n",
    "    evaluator -- instantiated evaluator object, e.g. MulticlassClassificationEvaluator()\n",
    "    result -- cross-validation fit-transformed data\n",
    "    \n",
    "    Output:\n",
    "    prints performance metrics f1, accuracy, and precision\n",
    "    '''\n",
    "    \n",
    "    print('f1: {}'.format(round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2)))\n",
    "    # accuracy and precision\n",
    "    correct_prediction_count, rows_all = result.filter(result.label == result.prediction).count(), result.count()\n",
    "    precision_count, rows_precision = result.filter((result.label == 1) & (result.prediction == 1)).count(), result.filter(result.label == 1).count()\n",
    "    print('accuracy: {:.0%} ({}/{}), and precision: {:.0%} ({}/{})'.format(\n",
    "        correct_prediction_count / rows_all, correct_prediction_count, rows_all,\n",
    "        precision_count / rows_precision, precision_count, rows_precision))\n",
    "    print('... ran {} min.\\n'.format(round((time.time() - start) / 60, 1)))\n",
    "\n",
    "# https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6\n",
    "#num_trees =  random.choice(list(range(50,500)))\n",
    "#depth = random.choice(list(range(2,10)))\n",
    "#criterion = random.choice(['gini','entropy'])\n",
    "# relevant read?: https://towardsdatascience.com/hyperparameters-part-ii-random-search-on-spark-77667e68b606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced search...\n",
    "#paramGrid_LogisticRegression = ParamGridBuilder() \\\n",
    "#    .addGrid(lr.regParam, [0.0]) \\\n",
    "#    .build()\n",
    "\n",
    "# parameter options inspired by https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6\n",
    "#paramGrid_RandomForestClassifier = ParamGridBuilder() \\\n",
    "#    .addGrid(rf.numTrees, [2]) \\\n",
    "#    .build()\n",
    "#    # rf.criterion,['gini','entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with gridsearch\n",
    "counter = 1\n",
    "best_f1 = 0\n",
    "\n",
    "for estimator in estimator_list: # see credentials above\n",
    "    print('{}/{}: {}'.format(counter, len(estimator_list), estimator.__class__.__name__))\n",
    "    counter += 1\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler, estimator])\n",
    "    \n",
    "    # create the CrossValidator object with the specific parameter grid and pipeline: \n",
    "    cv = create_cv(eval('paramGrid_' + estimator.__class__.__name__), pipeline)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = cv.fit(train)\n",
    "    result = model.transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    f1 = round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = round(evaluator.evaluate(result, {evaluator.metricName: 'f1'}), 2)\n",
    "        model_best = model.bestModel\n",
    "    evaluate_model(evaluator, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_f1)\n",
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to files: can use it without running the training job first\n",
    "# https://stackoverflow.com/questions/29255145/what-is-the-right-way-to-save-load-models-in-spark-pyspark?noredirect=1&lq=1\n",
    "model_best.write().overwrite().save('pyspark_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52625639/unable-to-load-logistic-regression-model-in-spark-2-x\n",
    "model_trained = PipelineModel.load('pyspark_trained_model')\n",
    "\n",
    "# inspired by https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines\n",
    "prediction = model_trained.transform(validate.columns) # .select(validate.drop('label'))\n",
    "selected = prediction.select(prediction.drop('features_unscaled', 'features', 'rawPrediction').columns)\n",
    "#selected.collect()\n",
    "#for row in selected.collect():\n",
    "#    userId, gender, days_listened, songs_per_day, d_female, prob, prediction = row  # type: ignore\n",
    "#    print(userId, prob[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad results with the small dataset do not mean no hope. More data is expected to be more helpful for better results than improvements in terms of parameter choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate results\n",
    "prediction = model_trained.transform(validate)\n",
    "evaluate_model(evaluator, result=prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on new data\n",
    "In a real-world application we __should leave out the newest data, train a model on the remaining data and test with the newest data__. This can be extended rolling the windows backwards.\n",
    "\n",
    "Simplification used / out-of-scope of this project: Here we include all history for model training. This is a flawed approach, however sufficient for this toy example. Why flawed? We mislead the inference with the assumption all customers who did no churn yet will not churn. Example: A customer who churns the next day should be labeled as _churn_. To know the label we need to leave out the newest data for the features to produce accurate labels.\n",
    "\n",
    "__Here we apply the best model found to the full dataset.__ This allows to provide a churn probability for all customers in the Sparkify Callcenter Dashboard web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_all = model_trained.transform(df_model)\n",
    "prediction_all = prediction_all.select(prediction.drop('features_unscaled', 'features', 'rawPrediction').columns)\n",
    "evaluate_model(evaluator, result=prediction_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkify = prediction_all.toPandas()\n",
    "sparkify['probability'] = sparkify.probability.apply(lambda x: x[1])\n",
    "sparkify['userId'] = sparkify.userId.apply(int)\n",
    "\n",
    "# write to database as app input\n",
    "engine = create_engine('sqlite:///sparkify.db')\n",
    "sparkify.to_sql('user_table', engine, index=False, if_exists='replace')\n",
    "\n",
    "print('Check database content... table exists and has entries:', \n",
    "      pd.read_sql('SELECT * FROM user_table', \n",
    "                  con=engine).shape[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkify.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given low amount of features: hardly able to predict churn\n",
    "print(np.corrcoef(sparkify.label, sparkify.probability)[0,1])\n",
    "plt.scatter(x=sparkify.label, y=sparkify.probability);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for row in selected.collect():\n",
    "#    userId, gender, days_listened, songs_per_day, d_female, prob, prediction = row  # type: ignore\n",
    "#    print(userId, prob[1], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rf.explainParams())\n",
    "#cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare against:\n",
    "# https://github.com/stephanieirvine/Udacity-Data-Scientist-Nanodegree/blob/main/Project%204/Sparkify.ipynb\n",
    "# blog: https://medium.com/swlh/predicting-churn-with-pyspark-4c8edc8a19e0\n",
    "\n",
    "# https://towardsdatascience.com/predicting-customer-churn-using-pyspark-6a78a78a8412"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
